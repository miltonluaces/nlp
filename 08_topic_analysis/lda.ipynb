{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.Admin.Standard import *\n",
    "import random\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "import gensim\n",
    "from gensim import corpora \n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "\n",
    "parser = English()\n",
    "def Tokenize(text):\n",
    "    ldaTokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace(): continue\n",
    "        elif token.like_url: ldaTokens.append('URL')\n",
    "        elif token.orth_.startswith('@'): ldaTokens.append('SCREEN_NAME')\n",
    "        else: ldaTokens.append(token.lower_)\n",
    "    return ldaTokens\n",
    "\n",
    "def GetLemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None: return word\n",
    "    else: return lemma\n",
    "    \n",
    "def PreProcess(text):\n",
    "    tokens = Tokenize(text)\n",
    "    tokens = [t for t in tokens if len(t) > 4]\n",
    "    tokens = [t for t in tokens if t not in en_stop]\n",
    "    tokens = [GetLemma(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "data = []\n",
    "with open(txtPath + 'dataset.txt') as f:\n",
    "    for line in f:\n",
    "        tokens = PreProcess(line)\n",
    "        if(random.random() > 0.99): print(tokens); data.append(tokens)\n",
    "\n",
    "\n",
    "dict = Dictionary(data)\n",
    "corpus = [dict.doc2bow(text) for text in data]\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dict.save('dict.gensim')\n",
    "\n",
    "numTopics = 5\n",
    "ldamodel = LdaModel(corpus, num_topics = numTopics, id2word=dict, passes=15)\n",
    "ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "print('\\nTopics:')\n",
    "for topic in topics: print(topic)\n",
    "\n",
    "newDoc = 'Practical Bayesian Optimization of Machine Learning Algorithms'\n",
    "newDoc = PreProcess(newDoc)\n",
    "newDocBow= dict.doc2bow(newDoc)\n",
    "print(newDocBow)\n",
    "topics = ldamodel.get_document_topics(newDocBow)\n",
    "print('\\nTopics:')\n",
    "print(topics)\n",
    "\n",
    "dict = Dictionary.load('dict.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda = LdaModel.load('model5.gensim')\n",
    "ldaDisplay = pyLDAvis.gensim.prepare(lda, corpus, dict, sort_topics=False)\n",
    "pyLDAvis.show(ldaDisplay)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#def GetLemma2(word):\n",
    "    #return WordNetLemmatizer().lemmatize(word)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

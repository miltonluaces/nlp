{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import distance \n",
    "\n",
    "# SIMILARITY SCORES\n",
    "\n",
    "# Important Paths\n",
    "fixtures = os.path.join(os.getcwd(), \"fixtures\")\n",
    "products = os.path.join(fixtures, \"products\")\n",
    "\n",
    "# Module Constants\n",
    "googId   = 'http://www.google.com/base/feeds/snippets'\n",
    "\n",
    "# Create a generator to load data from the products data source.\n",
    "def LoadData(name):\n",
    "    with open(os.path.join(products, name), 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader: yield row\n",
    "\n",
    "def GoogleKey(key):\n",
    "    return os.path.join(googId, key)\n",
    "\n",
    "# Load Datasets into Memory\n",
    "amazon  = list(LoadData('amazon.csv'))\n",
    "google  = list(LoadData('google.csv'))\n",
    "mapping = list(LoadData('perfect_mapping.csv'))\n",
    "\n",
    "# Datasets contents\n",
    "for name, dataset in (('Amazon', amazon), ('Google Shopping', google)):\n",
    "    print(\"{} dataset contains {} records\".format(name, len(dataset)))\n",
    "    print( \"Record keys: {}\\n\".format(\", \".join(dataset[0].keys())))\n",
    "\n",
    "# Datasets contents matching\n",
    "print( \"There are {} matching records to link\".format(len(mapping)))\n",
    "\n",
    "# Convert dataset to records indexed by their Id and show dumps (examples)\n",
    "amazon  = dict((v['id'], v) for v in amazon)\n",
    "google  = dict((v['id'], v) for v in google)\n",
    "X = amazon['b0000c7fpt']\n",
    "Y = google[GoogleKey('17175991674191849246')]\n",
    "print(json.dumps(X, indent=2))\n",
    "print(json.dumps(Y, indent=2))\n",
    "\n",
    "# Typographic Distances\n",
    "print(distance.levenshtein(\"lenvestein\", \"levenshtein\"))\n",
    "print(distance.hamming(\"hamming\", \"hamning\"))\n",
    "\n",
    "# Example: Compare glyphs, syllables, or phonemes \n",
    "t1 = (\"de\", \"ci\", \"si\", \"ve\")\n",
    "t2 = (\"de\", \"ri\", \"si\", \"ve\")\n",
    "print(distance.levenshtein(t1, t2))\n",
    "\n",
    "# Sentence Comparison\n",
    "sent1 = \"The quick brown fox jumped over the lazy dogs.\"\n",
    "sent2 = \"The lazy foxes are jumping over the crazy Dog.\"\n",
    "print(distance.nlevenshtein(sent1.split(), sent2.split(), method=1))\n",
    "\n",
    "# Normalization\n",
    "print(distance.hamming(\"fat\", \"cat\", normalized=True))\n",
    "print(distance.nlevenshtein(\"abc\", \"acd\", method=1))  # shortest alignment\n",
    "print(distance.nlevenshtein(\"abc\", \"acd\", method=2))  # longest alignment\n",
    "\n",
    "# Set measures\n",
    "print(distance.sorensen(\"decide\", \"resize\"))\n",
    "print(distance.jaccard(\"decide\", \"resize\"))\n",
    "\n",
    "\n",
    "# PROCESSED TEXT SCORE\n",
    "\n",
    "# Returns a similarity vector of match scores: [name_score, description_score, manufacturer_score, price_score]\n",
    "def Tokenize(sent):\n",
    "    lemmatizer = nltk.WordNetLemmatizer() \n",
    "    for token in nltk.wordpunct_tokenize(sent):\n",
    "        token = token.lower()\n",
    "        yield lemmatizer.lemmatize(token)\n",
    "\n",
    "def NormalizedJaccard(*args):\n",
    "    try: return distance.jaccard(*[Tokenize(arg) for arg in args])\n",
    "    except UnicodeDecodeError: return 0.0\n",
    "\n",
    "print(NormalizedJaccard(sent1, sent2))\n",
    "\n",
    "\n",
    "# SIMILARITY VECTORS\n",
    "\n",
    "# Returns a similarity vector of match scores: [name_score, description_score, manufacturer_score, price_score]\n",
    "def Similarity(prod1, prod2):\n",
    "    pair  = (prod1, prod2)\n",
    "    names = [r.get('name', None) or r.get('title', None) for r in pair]\n",
    "    descr = [r.get('description') for r in pair]\n",
    "    manuf = [r.get('manufacturer') for r in pair]\n",
    "    price = [float(r.get('price')) for r in pair]\n",
    "    return [NormalizedJaccard(*names), NormalizedJaccard(*descr), NormalizedJaccard(*manuf), abs(1.0/(1+ (price[0] - price[1]))),]\n",
    "\n",
    "print(Similarity(X, Y))\n",
    "\n",
    "# WEIGHTED PAIRWISE MATCHING\n",
    "\n",
    "thres = 0.90\n",
    "weights   = (0.6, 0.1, 0.2, 0.1)\n",
    "\n",
    "matches = 0\n",
    "for azprod in amazon.values():\n",
    "    for googprod in google.values():\n",
    "        vector = Similarity(azprod, googprod)\n",
    "        score  = sum(map(lambda v: v[0]*v[1], zip(weights, vector)))\n",
    "        if score > thres:\n",
    "            matches += 1\n",
    "            print(\"{0:0.3f}: {1} {2}\".format(score, azprod['id'], googprod['id'].split(\"/\")[-1]))\n",
    "\n",
    "print(\"\\n{} matches discovered\".format(matches))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "\n",
    "d = edit_distance (\"rain\", \"shine\")\n",
    "print(d)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

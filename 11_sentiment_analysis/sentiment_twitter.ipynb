{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter data sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "import random\n",
    "import nltk\n",
    "#nltk.download('twitter_samples')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('stopwords')\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
       " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n",
       " '@97sides CONGRATS :)',\n",
       " 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "test_tweets = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "pos_tweets[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
       " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n",
       " '@97sides CONGRATS :)',\n",
       " 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\",\n",
       " '@Hegelbon That heart sliding into the waste basket. :(',\n",
       " '“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\\n\\nMe too',\n",
       " 'Dang starting next week I have \"work\" :(']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_tweets[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#FollowFriday',\n",
       " '@France_Inte',\n",
       " '@PKuchly57',\n",
       " '@Milipol_Paris',\n",
       " 'for',\n",
       " 'being',\n",
       " 'top',\n",
       " 'engaged',\n",
       " 'members',\n",
       " 'in',\n",
       " 'my',\n",
       " 'community',\n",
       " 'this',\n",
       " 'week',\n",
       " ':)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = twitter_samples.tokenized('positive_tweets.json')\n",
    "tweet = tweets[0]\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#FollowFriday', 'JJ'),\n",
       " ('@France_Inte', 'NNP'),\n",
       " ('@PKuchly57', 'NNP'),\n",
       " ('@Milipol_Paris', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('being', 'VBG'),\n",
       " ('top', 'JJ'),\n",
       " ('engaged', 'VBN'),\n",
       " ('members', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('community', 'NN'),\n",
       " ('this', 'DT'),\n",
       " ('week', 'NN'),\n",
       " (':)', 'NN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#FollowFriday',\n",
       " '@France_Inte',\n",
       " '@PKuchly57',\n",
       " '@Milipol_Paris',\n",
       " 'for',\n",
       " 'be',\n",
       " 'top',\n",
       " 'engage',\n",
       " 'member',\n",
       " 'in',\n",
       " 'my',\n",
       " 'community',\n",
       " 'this',\n",
       " 'week',\n",
       " ':)']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized\n",
    "\n",
    "lemmatize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sw_remove(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|''(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "sw_remove(tweet, stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "neg_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "pos_clean = []\n",
    "neg_clean = []\n",
    "\n",
    "for tokens in pos_tokens:\n",
    "    pos_clean.append(sw_remove(tokens, stop_words))\n",
    "\n",
    "for tokens in neg_tokens:\n",
    "    neg_clean.append(sw_remove(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw  : \t ['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "Clean :\t ['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "print('Raw  : \\t', pos_tokens[500])\n",
    "print('Clean :\\t', pos_clean[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw  : \t ['Can', 'u', 'feel', 'it', '?', ':(', '(:', '(', '#exo', 'http://t.co/ghsa262ORm']\n",
      "Clean :\t ['u', 'feel', ':(', '(:', '#exo']\n"
     ]
    }
   ],
   "source": [
    "print('Raw  : \\t', neg_tokens[500])\n",
    "print('Clean :\\t', neg_clean[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object generator at 0x0000022CAF193660>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generator(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "gen = generator(pos_clean)\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(':)', 3691),\n",
       " (':-)', 701),\n",
       " (':d', 658),\n",
       " ('thanks', 388),\n",
       " ('follow', 357),\n",
       " ('love', 333),\n",
       " ('...', 290),\n",
       " ('good', 283),\n",
       " ('get', 263),\n",
       " ('thank', 253)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = FreqDist(gen)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(tokens):\n",
    "    for tokens in tokens:\n",
    "        yield dict([token, True] for token in tokens)\n",
    "\n",
    "pos_tokens = get_tokens(pos_clean)\n",
    "neg_tokens = get_tokens(neg_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ds = [(token, \"Positive\") for token in pos_tokens]\n",
    "neg_ds = [(token, \"Negative\") for token in neg_tokens]\n",
    "\n",
    "ds = pos_ds + neg_ds\n",
    "random.shuffle(ds)\n",
    "\n",
    "split_index = 7000\n",
    "ds_train = ds[:split_index]\n",
    "ds_test = ds[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'think': True, 'hard': True, 'bed': True, ':p': True}, 'Positive'),\n",
       " ({'suspect': True,\n",
       "   'travel': True,\n",
       "   'widely': True,\n",
       "   'eventually': True,\n",
       "   'return': True,\n",
       "   'home': True,\n",
       "   'reaapearing': True,\n",
       "   'fridge': True,\n",
       "   ':-)': True},\n",
       "  'Positive'),\n",
       " ({'ah': True, 'jaysus': True, ':(': True}, 'Negative'),\n",
       " ({'phone': True,\n",
       "   'shit': True,\n",
       "   'always': True,\n",
       "   'run': True,\n",
       "   'memory': True,\n",
       "   ':(': True,\n",
       "   '...': True,\n",
       "   '2': True,\n",
       "   'many': True,\n",
       "   'nude': True},\n",
       "  'Negative'),\n",
       " ({'anyone': True,\n",
       "   'need': True,\n",
       "   'ride': True,\n",
       "   '#educampakl': True,\n",
       "   'tomorrow': True,\n",
       "   'leave': True,\n",
       "   'rotorua': True,\n",
       "   'early': True,\n",
       "   'morning': True,\n",
       "   'pick': True,\n",
       "   'people': True,\n",
       "   'way': True,\n",
       "   'like': True,\n",
       "   'come': True,\n",
       "   ':)': True},\n",
       "  'Positive'),\n",
       " ({':)': True, '☕': True, 'thank': True, 'rita': True}, 'Positive'),\n",
       " ({'laguna': True, ':(': True}, 'Negative'),\n",
       " ({'powys': True,\n",
       "   'close': True,\n",
       "   'shropshire': True,\n",
       "   'border': True,\n",
       "   ':)': True},\n",
       "  'Positive'),\n",
       " ({'morning': True,\n",
       "   '...': True,\n",
       "   'bacon': True,\n",
       "   'roll': True,\n",
       "   'friday': True,\n",
       "   ':d': True},\n",
       "  'Positive')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.996\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2050.5 : 1.0\n",
      "                      :) = True           Positi : Negati =    994.6 : 1.0\n",
      "                     sad = True           Negati : Positi =     33.3 : 1.0\n",
      "                  arrive = True           Positi : Negati =     24.4 : 1.0\n",
      "                    glad = True           Positi : Negati =     21.1 : 1.0\n",
      "                follower = True           Positi : Negati =     17.4 : 1.0\n",
      "                     x15 = True           Negati : Positi =     16.3 : 1.0\n",
      "               community = True           Positi : Negati =     15.7 : 1.0\n",
      "                followed = True           Negati : Positi =     15.0 : 1.0\n",
      "                     ugh = True           Negati : Positi =     13.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = NaiveBayesClassifier.train(ds_train)\n",
    "\n",
    "print(\"Accuracy : \", classify.accuracy(model, ds_test))\n",
    "print(model.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet1 = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "tokens = sw_remove(word_tokenize(tweet1))\n",
    "model.classify(dict([token, True] for token in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet2 = 'Congrats #SportStar on your 7th best goal from last season winning goal of the year :) #Baller #Topbin #oneofmanyworldies'\n",
    "tokens = sw_remove(word_tokenize(tweet2))\n",
    "model.classify(dict([token, True] for token in tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits & Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 64-bit",
   "language": "python",
   "name": "python38164bitc33de82c9da04edea88eb124459bf44a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

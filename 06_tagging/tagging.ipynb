{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "str = \"The glass is on the table\"\n",
    "words = word_tokenize(str)\n",
    "tags = pos_tag(words)\n",
    "print(tags)\n",
    "\n",
    "# List all nouns\n",
    "nouns = []\n",
    "for word, pos in tags:\n",
    "    if pos in ['NN', 'NNP']: nouns.append(word)\n",
    " \n",
    "print(nouns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name-entity tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Milton/NNP)\n",
      "  is/VBZ\n",
      "  working/VBG\n",
      "  in/IN\n",
      "  (GPE Accenture/NNP)\n",
      "  in/IN\n",
      "  (GPE Dublin/NNP))\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk import ne_chunk\n",
    "\n",
    "str = \"Milton is working in Accenture in Dublin\"\n",
    "\n",
    "tags = ne_chunk(pos_tag(word_tokenize(str)), binary=False)\n",
    "print(tags)\n",
    "\n",
    "# NERTagger from Stanford\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            print(namedEnt)\n",
    "            namedEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy && python -m spacy download en\n",
    "# pip install spacy && python -m spacy download en_core_web_lg\n",
    "\n",
    "from scipy import spatial\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Token\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "\n",
    "# Tokenizing\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp('Hello     World!')\n",
    "for token in doc:\n",
    "    print('\"' + token.text + '\"')\n",
    " \n",
    "# Word tagging\n",
    "doc = nlp(\"Next week I'll   be in Madrid.\")\n",
    "for token in doc:\n",
    "    print(\"\\ntext    : \" + token.text)\n",
    "    print(\"idx     : \" + str(token.idx))\n",
    "    print(\"lemma   : \" + token.lemma_)\n",
    "    print(\"isPunct : \" + str(token.is_punct))\n",
    "    print(\"isSpace : \" + str(token.is_space))\n",
    "    print(\"shape   : \" + str(token.shape_))\n",
    "    print(\"pos     : \" + str(token.pos_))\n",
    "    print(\"tag     : \" + token.tag_)\n",
    "\n",
    "# Sentence tagging\n",
    "doc = nlp(\"These are apples. These are oranges.\")\n",
    "for sent in doc.sents: print(sent)\n",
    "\n",
    "# Part-of-speech tagging\n",
    "doc = nlp(\"Next week I'll be in Madrid.\")\n",
    "print([(token.text, token.tag_) for token in doc])\n",
    "\n",
    "# Named entity recognition tagging\n",
    "doc = nlp(\"Next week I'll be in Madrid.\") \n",
    "for ent in doc.ents: print(ent.text, ent.label_)\n",
    "\n",
    "# In/Out/Begin (IOB) tagging\n",
    "doc = nlp(\"Next week I'll be in Madrid.\")\n",
    "iob = [(token.text, token.tag_, \"{0}-{1}\".format(token.ent_iob_, token.ent_type_) if token.ent_iob_ != 'O' else token.ent_iob_) for token in doc]\n",
    "print(iob)\n",
    "print(conlltags2tree(iob)) # Tree\n",
    "\n",
    "# Entity tagging\n",
    "doc = nlp(\"I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ\")\n",
    "for ent in doc.ents: print(ent.text, ent.label_)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "\n",
    "# Chunk tagging\n",
    "doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
    "for chunk in doc.noun_chunks: print(chunk.text, chunk.label_, chunk.root.text)\n",
    "\n",
    "# Dependency parsing\n",
    "for token in doc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))\n",
    "\n",
    "# Distance measuring\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n",
    "\n",
    "# Word vectors\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "print(nlp.vocab['banana'].vector)\n",
    "\n",
    "# Cosine similarity\n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "queen = nlp.vocab['queen'].vector\n",
    "king = nlp.vocab['king'].vector\n",
    " \n",
    "# Closest vector in the vocabulary (to the result of \"man\" - \"woman\" + \"queen\")\n",
    "maybe_king = man - woman + queen\n",
    "similarities = []\n",
    " \n",
    "for word in nlp.vocab:\n",
    "    if not word.has_vector: continue\n",
    "    similarity = cosine_similarity(maybe_king, word.vector)\n",
    "    similarities.append((word, similarity))\n",
    "\n",
    "similarities = sorted(similarities, key=lambda item: -item[1])\n",
    "print([w[0].text for w in similarities[:10]])\n",
    "\n",
    "# Computing similarity\n",
    "banana = nlp.vocab['banana']\n",
    "dog = nlp.vocab['dog']\n",
    "fruit = nlp.vocab['fruit']\n",
    "animal = nlp.vocab['animal']\n",
    "\n",
    "print(dog.similarity(animal), dog.similarity(fruit)) # 0.6618534 0.23552845\n",
    "print(banana.similarity(fruit), banana.similarity(animal)) # 0.67148364 0.2427285\n",
    "\n",
    "target = nlp(\"Cats are beautiful animals.\")\n",
    " \n",
    "doc1 = nlp(\"Dogs are awesome.\")\n",
    "doc2 = nlp(\"Some gorgeous creatures are felines.\")\n",
    "doc3 = nlp(\"Dolphins are swimming mammals.\")\n",
    " \n",
    "print(target.similarity(doc1))  # 0.8901765218466683\n",
    "print(target.similarity(doc2))  # 0.9115828449161616\n",
    "print(target.similarity(doc3))  # 0.7822956752876101\n",
    "\n",
    "# Sentiment Analysis\n",
    "sentAnalyzer = SentimentIntensityAnalyzer()\n",
    "def polarity_scores(doc):\n",
    "    return sentAnalyzer.polarity_scores(doc.text)\n",
    " \n",
    "Doc.set_extension('polarity_scores', getter=polarity_scores)\n",
    " \n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(\"Really Whaaat event apple nice! it!\")\n",
    "print(doc._.polarity_scores)\n",
    "# {'neg': 0.0, 'neu': 0.596, 'pos': 0.404, 'compound': 0.5242}\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "print(nlp.pipeline) \n",
    " \n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('N'): return 'n'\n",
    "    elif tag.startswith('V'): return 'v' \n",
    "    elif tag.startswith('J'): return 'a'\n",
    "    elif tag.startswith('R'): return 'r'\n",
    "    return None\n",
    " \n",
    " \n",
    "class WordnetPipeline(object):\n",
    "    def __init__(self, nlp):\n",
    "        Token.set_extension('synset', default=None)\n",
    " \n",
    "    def __call__(self, doc):\n",
    "        for token in doc:\n",
    "            wn_tag = penn_to_wn(token.tag_)\n",
    "            if wn_tag is None: continue\n",
    " \n",
    "            ss = wn.synsets(token.text, wn_tag)[0]\n",
    "            token._.set('synset', ss)\n",
    " \n",
    "        return doc\n",
    " \n",
    " \n",
    "nlp = spacy.load('en')\n",
    "wn_pipeline = WordnetPipeline(nlp)\n",
    "nlp.add_pipe(wn_pipeline, name='wn_synsets')\n",
    "doc = nlp(\"Paris is the awesome capital of France.\")\n",
    " \n",
    "for token in doc:\n",
    "    print(token.text, \"-\", token._.synset)\n",
    "\n",
    "print(nlp.pipeline)\n",
    "print(ent.text, ent.label_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk import word_tokenize\n",
    "\n",
    "str = \"A passenger plane has crashed shortly after take-off from Kyrgyzstan's capital, Bishkek, killing a large number of those on board. The head of Kyrgyzstan's civil aviation authority said that out of about 90 passengers and crew, only about 20 people have survived. The Itek Air Boeing 737 took off bound for Mashhad, in north-eastern Iran, but turned round some 10 minutes later.\"\n",
    "\n",
    "tagger = StanfordPOSTagger('FileModels/Postagger/models/english-bidirectional-distsim.tagger', 'FileModels/Postagger/stanford-postagger.jar')\n",
    "tokens = word_tokenize(str)\n",
    "tags = tagger.tag(tokens)\n",
    "print(tags)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
